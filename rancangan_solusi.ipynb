{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "msrp = pd.read_csv('data/processed/msrp_train_translated_id.txt', sep='\\t', quotechar='^')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect(row):\n",
    "    print(row['sentence1'])\n",
    "    print(row['sentence2'])\n",
    "    print(row['label'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lampu keamanan juga telah dipasang dan polisi telah menyapu lahan untuk perangkap booby.\n",
      "Lampu keamanan juga telah dipasang di gudang di dekat gerbang depan.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "inspect(msrp.loc[101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.A. Jaccard Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpstemmer import MPStemmer\n",
    "import string\n",
    "\n",
    "stemmer = MPStemmer()\n",
    "\n",
    "def preprocess_sentence(list_of_sentences):\n",
    "    preprocessed = []\n",
    "    for sentence in list_of_sentences:\n",
    "        sentence = sentence.translate(str.maketrans('','',string.punctuation))\n",
    "        print(sentence)\n",
    "        sentence = sentence.lower()\n",
    "        sentence = stemmer.stem_kalimat(sentence)\n",
    "        print(sentence)\n",
    "        preprocessed.append(sentence)\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = msrp.iloc[101:102].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lampu keamanan juga telah dipasang dan polisi telah menyapu lahan untuk perangkap booby\n",
      "lampu aman juga telah pasang dan polisi telah sapu lahan untuk perangkap booby\n",
      "Lampu keamanan juga telah dipasang di gudang di dekat gerbang depan\n",
      "lampu aman juga telah pasang di gudang di dekat gerbang depan\n"
     ]
    }
   ],
   "source": [
    "instance['preprocessed_sentence1'] = preprocess_sentence(instance['sentence1'])\n",
    "instance['preprocessed_sentence2'] = preprocess_sentence(instance['sentence2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "def extract_feat_1(df_train, df_test, inductive=True):\n",
    "    if inductive:\n",
    "        all_sentences = np.concatenate([df_train['preprocessed_sentence1'], df_train['preprocessed_sentence2']])\n",
    "    else:\n",
    "        all_sentences = np.concatenate([df_train['preprocessed_sentence1'], df_train['preprocessed_sentence2'],\n",
    "                                        df_test['preprocessed_sentence1'], df_test['preprocessed_sentence2']])\n",
    "    vec = CountVectorizer(binary=True)\n",
    "    vec.fit(all_sentences)\n",
    "    \n",
    "    X1_train = vec.transform(df_train['preprocessed_sentence1']).toarray()\n",
    "    X2_train = vec.transform(df_train['preprocessed_sentence2']).toarray()\n",
    "    print(X1_train)\n",
    "    print(X2_train)\n",
    "    \n",
    "    X1_test = vec.transform(df_test['preprocessed_sentence1']).toarray()\n",
    "    X2_test = vec.transform(df_test['preprocessed_sentence2']).toarray()\n",
    "    \n",
    "    feat_1_train = [jaccard_score(x1, x2, average='binary') for x1, x2 in zip(X1_train, X2_train)]\n",
    "    feat_1_test = [jaccard_score(x1, x2, average='binary') for x1, x2 in zip(X1_test, X2_test)]\n",
    "    print(feat_1_train)\n",
    "    \n",
    "    df_feat_train = pd.DataFrame(feat_1_train, columns=['Jaccard_Score'])\n",
    "    df_feat_train['label'] = df_train['label']\n",
    "    \n",
    "    df_feat_test = pd.DataFrame(feat_1_test, columns=['Jaccard_Score'])\n",
    "    df_feat_test['label'] = df_test['label']\n",
    "    \n",
    "    return df_feat_train, df_feat_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 1 1]]\n",
      "[[1 0 0 1 1 1 1 1 1 0 1 1 0 0 0 1 0]]\n",
      "[0.29411764705882354]\n"
     ]
    }
   ],
   "source": [
    "train, test = extract_feat_1(instance, instance, inductive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.B. SMATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_2_b(row):\n",
    "    print(row['amr1'])\n",
    "    print(row['amr2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "amr = pd.read_csv('data/processed/amr_msrp_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# ::id 101\n",
      "# ::annotator indoamrbart-mbart-fted\n",
      "# ::snt Lampu keamanan juga telah dipasang dan polisi telah menyapu lahan untuk perangkap booby.\n",
      "(z0 / dan\n",
      "    :op1 (z1 / pasang-01\n",
      "             :ARG1 (z2 / lampu\n",
      "                       :mod (z3 / keamanan))\n",
      "             :time (z4 / juga))\n",
      "    :op2 (z5 / menyapu-01\n",
      "             :ARG0 (z6 / polisi)\n",
      "             :ARG1 (z7 / lahan)\n",
      "             :purpose (z8 / perangkap\n",
      "                          :mod (z9 / booby))))\n",
      "# ::id 101\n",
      "# ::annotator indoamrbart-mbart-fted\n",
      "# ::snt Lampu keamanan juga telah dipasang di gudang di dekat gerbang depan.\n",
      "(z0 / pasang-01\n",
      "    :ARG0 (z1 / orang\n",
      "              :ARG0-of (z2 / memiliki-peran-org-91\n",
      "                           :ARG2 (z3 / polisi)))\n",
      "    :ARG1 (z4 / lampu\n",
      "              :mod (z5 / keamanan))\n",
      "    :location (z6 / gudang\n",
      "                  :location (z7 / dekat\n",
      "                                :op1 (z8 / pintu\n",
      "                                         :mod (z9 / depan))))\n",
      "    :mod (z10 / juga))\n"
     ]
    }
   ],
   "source": [
    "inspect_2_b(amr.loc[101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_amr_entries_from_df(df, column_name):\n",
    "    data = '\\n\\n'.join(df[column_name].tolist())\n",
    "    \n",
    "    lines = [l for l in data.splitlines() if not l.startswith('#')]\n",
    "    data = '\\n'.join(lines)\n",
    "    \n",
    "    entries = data.split('\\n\\n')\n",
    "    entries = [e.strip() for e in entries]\n",
    "    entries = [e for e in entries if e]\n",
    "    \n",
    "    return entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from amrlib.evaluate.smatch_enhanced import compute_smatch\n",
    "\n",
    "amr1_entries = load_amr_entries_from_df(amr.iloc[101:102], 'amr1')\n",
    "amr2_entries = load_amr_entries_from_df(amr.iloc[101:102], 'amr2')\n",
    "\n",
    "assert len(amr1_entries) == len(amr2_entries), \"Number of amr 1 and amr 2 must be the same\"\n",
    "\n",
    "smatch_scores = []\n",
    "\n",
    "i = 0\n",
    "for amr1_graph, amr2_graph in zip(amr1_entries, amr2_entries):\n",
    "    amr1_graph = [amr1_graph]\n",
    "    amr2_graph = [amr2_graph]\n",
    "    precision, recall, f_score = compute_smatch(amr1_graph, amr2_graph)\n",
    "    smatch_scores.append(f_score)\n",
    "    i = i+1\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.380952380952381]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smatch_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpstemmer import MPStemmer\n",
    "import string\n",
    "import penman\n",
    "\n",
    "stemmer = MPStemmer()\n",
    "\n",
    "\n",
    "def extract_concepts(amr):\n",
    "    graph = penman.decode(amr)\n",
    "    words = [node[2] for node in graph.instances()]\n",
    "    # words = [word.replace('-', '').rstrip('0123456789') for word in words]\n",
    "    for triple in graph.triples:\n",
    "        if triple[1].startswith((\":op\", \":time\")):\n",
    "            if triple[2][0] == \"z\" and not triple[2][1:].isdigit():\n",
    "                words.append(triple[2])\n",
    "            elif triple[2][0] != \"z\":\n",
    "                words.append(triple[2])\n",
    "        elif triple[1].startswith(\":wiki\") and triple[2] != \"-\":\n",
    "            words.append(triple[2])\n",
    "    return words\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import jaccard_score   \n",
    "\n",
    "def extract_feat(df_train, df_test, inductive=True):\n",
    "    if inductive:\n",
    "        all_sentences = np.concatenate([df_train['concepts_amr1'], df_train['concepts_amr2']])\n",
    "    else:\n",
    "        all_sentences = np.concatenate([df_train['concepts_amr1'], df_train['concepts_amr2'],\n",
    "                                        df_test['concepts_amr1'], df_test['concepts_amr2']])\n",
    "    vec = CountVectorizer(binary=True, token_pattern=r'\\b\\w[\\w-]*\\b')\n",
    "    vec.fit(all_sentences)\n",
    "    \n",
    "    X1_train = vec.transform(df_train['concepts_amr1']).toarray()\n",
    "    X2_train = vec.transform(df_train['concepts_amr2']).toarray()\n",
    "    print(X1_train)\n",
    "    print(X2_train)\n",
    "    \n",
    "    X1_test = vec.transform(df_test['concepts_amr1']).toarray()\n",
    "    X2_test = vec.transform(df_test['concepts_amr2']).toarray()\n",
    "    \n",
    "    feat_1_train = [jaccard_score(x1, x2, average='binary') for x1, x2 in zip(X1_train, X2_train)]\n",
    "    feat_1_test = [jaccard_score(x1, x2, average='binary') for x1, x2 in zip(X1_test, X2_test)]\n",
    "    print(feat_1_train)\n",
    "    \n",
    "    df_feat_train = pd.DataFrame(feat_1_train, columns=['Jaccard_Score'])\n",
    "    df_feat_train['label'] = df_train['label']\n",
    "    df_feat_train['feat_smatch'] = df_train['feat_smatch']\n",
    "    \n",
    "    df_feat_test = pd.DataFrame(feat_1_test, columns=['Jaccard_Score'])\n",
    "    df_feat_test['label'] = df_test['label']\n",
    "    df_feat_test['feat_smatch'] = df_test['feat_smatch']\n",
    "    \n",
    "    return df_feat_train, df_feat_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_2_c = amr.iloc[101:102].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dan', 'pasang-01', 'lampu', 'keamanan', 'juga', 'menyapu-01', 'polisi', 'lahan', 'perangkap', 'booby']\n",
      "['pasang-01', 'orang', 'memiliki-peran-org-91', 'polisi', 'lampu', 'keamanan', 'gudang', 'dekat', 'pintu', 'depan', 'juga']\n"
     ]
    }
   ],
   "source": [
    "instance_2_c['concepts_amr1'] = instance_2_c['amr1'].apply(extract_concepts)\n",
    "instance_2_c['concepts_amr2'] = instance_2_c['amr2'].apply(extract_concepts)\n",
    "instance_2_c['concepts_amr1'] = instance_2_c['concepts_amr1'].apply(' '.join)\n",
    "instance_2_c['concepts_amr2'] = instance_2_c['concepts_amr2'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dan pasang-01 lampu keamanan juga menyapu-01 polisi lahan perangkap booby\n",
      "pasang-01 orang memiliki-peran-org-91 polisi lampu keamanan gudang dekat pintu depan juga\n"
     ]
    }
   ],
   "source": [
    "print(instance_2_c.loc[101]['concepts_amr1'])\n",
    "print(instance_2_c.loc[101]['concepts_amr2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = extract_feat(instance_2_c, instance_2_c, inductive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.A.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "      <th>preprocessed_sentence1</th>\n",
       "      <th>preprocessed_sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>Lampu keamanan juga telah dipasang dan polisi ...</td>\n",
       "      <td>Lampu keamanan juga telah dipasang di gudang d...</td>\n",
       "      <td>0</td>\n",
       "      <td>lampu aman juga telah pasang dan polisi telah ...</td>\n",
       "      <td>lampu aman juga telah pasang di gudang di deka...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence1  \\\n",
       "101  Lampu keamanan juga telah dipasang dan polisi ...   \n",
       "\n",
       "                                             sentence2  label  \\\n",
       "101  Lampu keamanan juga telah dipasang di gudang d...      0   \n",
       "\n",
       "                                preprocessed_sentence1  \\\n",
       "101  lampu aman juga telah pasang dan polisi telah ...   \n",
       "\n",
       "                                preprocessed_sentence2  \n",
       "101  lampu aman juga telah pasang di gudang di deka...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_3_a= amr.iloc[101:102].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_3_a['concepts_amr1'] = instance_3_a['amr1'].apply(extract_concepts)\n",
    "instance_3_a['concepts_amr2'] = instance_3_a['amr2'].apply(extract_concepts)\n",
    "instance_3_a['concepts_amr1'] = instance_3_a['concepts_amr1'].apply(' '.join)\n",
    "instance_3_a['concepts_amr2'] = instance_3_a['concepts_amr2'].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dan pasang-01 lampu keamanan juga menyapu-01 polisi lahan perangkap booby\n",
      "pasang-01 orang memiliki-peran-org-91 polisi lampu keamanan gudang dekat pintu depan juga\n"
     ]
    }
   ],
   "source": [
    "print(instance_3_a.iloc[0]['concepts_amr1'])\n",
    "print(instance_3_a.iloc[0]['concepts_amr2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import vstack, hstack\n",
    "\n",
    "def extr_feat(df_train, df_test, factor=True, n_comp=100, inductive=True):\n",
    "    df_train = df_train.copy()\n",
    "    df_test = df_test.copy()\n",
    "    if inductive:\n",
    "        all_sent = np.concatenate([df_train['concepts_amr1'], df_train['concepts_amr2']])\n",
    "    else:\n",
    "        all_sent = np.concatenate([df_train['concepts_amr1'], df_train['concepts_amr2'],\n",
    "                                    df_test['concepts_amr1'], df_test['concepts_amr2']])\n",
    "    vec = TfidfVectorizer(token_pattern=r'\\b\\w[\\w-]*\\b')\n",
    "    vec.fit(all_sent)\n",
    "\n",
    "    X1_train = vec.transform(df_train['concepts_amr1'])\n",
    "    # print(X1_train.toarray())\n",
    "    X2_train = vec.transform(df_train['concepts_amr2'])\n",
    "    X_train = vstack((X1_train, X2_train))\n",
    "    # print(X_train.shape)\n",
    "    X1_test = vec.transform(df_test['concepts_amr1'])\n",
    "    X2_test = vec.transform(df_test['concepts_amr1'])\n",
    "\n",
    "    print(\"vocab size:\" ,len(vec.vocabulary_))\n",
    "    if (factor):\n",
    "        n_components = math.floor(n_comp * len(vec.vocabulary_))\n",
    "    else:\n",
    "        n_components = n_comp\n",
    "    lsa = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    X_train = lsa.fit_transform(X_train)\n",
    "    # print(X_train.shape)\n",
    "    X_train = np.vsplit(X_train, 2)\n",
    "    sum_res = X_train[0] + X_train[1]\n",
    "    # print(X_train[0])\n",
    "    # print(X_train[1])\n",
    "    diff_res = np.abs(X_train[0] - X_train[1])\n",
    "    feat_1_train = np.column_stack((sum_res, diff_res))\n",
    "    lsa_df_train = pd.DataFrame(feat_1_train.tolist(), columns=[f'f{i}' for i in range(feat_1_train.shape[1])], index=df_train.index)\n",
    "    # lsa_df_train['label'] = df_train['label']\n",
    "    print(feat_1_train)\n",
    "    X1_test = lsa.transform(X1_test)\n",
    "    X2_test = lsa.transform(X2_test)\n",
    "    sum_res = X1_test + X2_test\n",
    "    diff_res = np.abs(X1_test - X2_test)\n",
    "    feat_1_test = np.column_stack((sum_res, diff_res))\n",
    "    lsa_df_test = pd.DataFrame(feat_1_test.tolist(), columns=[f'f{i}' for i in range(feat_1_test.shape[1])], index=df_test.index)\n",
    "    # lsa_df_test['label'] = df_test['label']\n",
    "    \n",
    "    return lsa_df_train, lsa_df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 41\n",
      "[[ 0.46316594 -0.70505257 -0.47469192 -0.25108916]\n",
      " [ 0.6940693   0.47880313  0.22445288 -0.48850411]]\n",
      "[[ 0.6115788  -0.56750989  0.4727312   0.28360027]\n",
      " [ 0.66464144  0.51352621 -0.33858422  0.42415011]]\n"
     ]
    }
   ],
   "source": [
    "train, test = extr_feat(instance_3_a, instance_3_a, factor=True, n_comp=0.5, inductive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 125\n",
      "[[ 3.23017042e-01  6.88409879e-01 -9.80767675e-01 -1.11356075e+00\n",
      "  -8.88529300e-02 -1.34762002e-01  2.67368222e-01  1.42054712e-01\n",
      "   1.22844301e-01  9.18883274e-02  7.61860503e-03  6.08943718e-02\n",
      "   1.70787784e-02  6.16997418e-03  9.32053389e-02  1.06340122e-01]\n",
      " [ 1.04952311e+00 -1.40112162e-01 -7.08967489e-02 -1.30320959e-01\n",
      "   2.68625995e-01 -2.81825081e-01 -6.46647205e-01 -9.52837034e-01\n",
      "   9.63392207e-02  7.38595214e-03  3.18781827e-02  1.84350970e-02\n",
      "   8.04234272e-02  3.30210828e-02  7.43508950e-02  1.49751620e-01]\n",
      " [ 8.56483788e-01 -7.23097221e-02  5.26473529e-01  5.30682893e-02\n",
      "  -1.12142490e-01 -4.95616983e-01  1.22133812e+00 -1.94090747e-01\n",
      "   1.64244954e-01  9.91003221e-02  7.75966297e-02  1.37849438e-03\n",
      "   1.60610290e-03  5.03545592e-02  1.10087454e-01  2.17652753e-02]\n",
      " [ 1.16022720e+00 -2.79136165e-01  1.69202572e-01 -3.83518286e-03\n",
      "   4.44371597e-02 -4.25562444e-01 -4.68846620e-01  1.01874604e+00\n",
      "   1.92632432e-01  2.50012869e-02  6.26005857e-02  3.92311279e-02\n",
      "   4.02772401e-02  5.18075510e-02  5.05855954e-02  3.11580451e-01]\n",
      " [ 8.33424883e-01 -3.04540877e-01  1.60033954e-01 -2.78669305e-01\n",
      "  -3.44132826e-01  1.36059087e+00  1.29760710e-01 -1.48473615e-03\n",
      "   1.66577718e-01  1.72318249e-02  4.47471805e-02  3.86102251e-02\n",
      "   3.36475673e-02  1.17911419e-01  5.48314606e-02  9.72762929e-02]\n",
      " [ 7.04191360e-02  1.28727710e+00  1.06727299e+00 -2.46009054e-01\n",
      "   5.43810862e-02  1.13647947e-01 -2.89008227e-01  1.15002464e-02\n",
      "   5.39789195e-03  4.19373292e-02  5.54034531e-02  1.77484467e-02\n",
      "   1.68961332e-03  1.87598988e-02  1.82970428e-02  1.78199272e-02]\n",
      " [ 6.32301088e-01  8.22188482e-01 -6.97563294e-01  1.18087528e+00\n",
      "  -3.39039531e-02  2.15399515e-01  9.67875975e-02  2.99805539e-02\n",
      "   2.65099023e-01  9.23750988e-02  4.68281981e-02  5.79275640e-02\n",
      "   1.29645993e-02  5.89708005e-02  5.28577756e-02  4.84507269e-02]\n",
      " [ 4.42690448e-02 -2.70506400e-02 -2.41216372e-02 -6.05061304e-02\n",
      "   1.55279704e+00  3.22721611e-01  2.78162774e-01  1.34793514e-01\n",
      "   2.70499010e-02  6.72596228e-03  4.95594755e-03  7.85592269e-03\n",
      "   2.08989250e-02  1.58412887e-02  5.19264019e-02  7.63453963e-02]]\n"
     ]
    }
   ],
   "source": [
    "train, test = extr_feat(instance_3_a, instance_3_a, factor=False, n_comp=8, inductive=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>1.622212</td>\n",
       "      <td>-4.440892e-16</td>\n",
       "      <td>3.330669e-16</td>\n",
       "      <td>1.169799</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           f0            f1            f2        f3\n",
       "101  1.622212 -4.440892e-16  3.330669e-16  1.169799"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_two_amr(amr1, amr2):\n",
    "    g1 = penman.decode(amr1)\n",
    "    g2 = penman.decode(amr2)\n",
    "    g2 = [('y' + triple[0][1:], triple[1], triple[2]) for triple in g2.triples]\n",
    "    for triple in g2.copy():\n",
    "        if (triple[2].startswith('z') and triple[2][1:].isdigit()):\n",
    "            g2.append((triple[0], triple[1], 'y' + triple[2][1:]))\n",
    "            g2.remove(triple)\n",
    "    for triple in g2:\n",
    "        if triple not in g1.triples:\n",
    "            g1.triples.append(triple)\n",
    "            \n",
    "    words = []\n",
    "    for triple in g1.instances():\n",
    "        ada = False\n",
    "        for tup in words:\n",
    "            if tup[1] == triple[2]:\n",
    "                ada = True\n",
    "        if not ada:\n",
    "            words.append((triple[0],triple[2]))\n",
    "    words = dict(words)\n",
    "    graph = g1.triples.copy()\n",
    "    graph_merged = graph.copy()\n",
    "\n",
    "    for triple in graph:\n",
    "        if triple[0] not in words:\n",
    "            for inner in graph:\n",
    "                if triple[0]==inner[0] and inner[1]==':instance':\n",
    "                    for v,i in words.items():\n",
    "                        if i==inner[2]:\n",
    "                            change = triple[0]\n",
    "                            to = v  \n",
    "                            temp = graph_merged.copy()\n",
    "                            for supinner in temp:\n",
    "                                if supinner[0]==change:\n",
    "                                    graph_merged.append((to, supinner[1], supinner[2]))\n",
    "                                    graph_merged.remove(supinner)\n",
    "                            temp = graph_merged.copy()\n",
    "                            for supinner in temp:    \n",
    "                                if supinner[2]==change:\n",
    "                                    graph_merged.append((supinner[0], supinner[1], to))\n",
    "                                    graph_merged.remove(supinner)\n",
    "    return graph_merged "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_3_a['merged_amr'] = instance_3_a.apply(lambda row: merge_two_amr(row['amr1'], row['amr2']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amr1</th>\n",
       "      <th>amr2</th>\n",
       "      <th>concepts_amr1</th>\n",
       "      <th>concepts_amr2</th>\n",
       "      <th>merged_amr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td># ::id 101\\n# ::annotator indoamrbart-mbart-ft...</td>\n",
       "      <td># ::id 101\\n# ::annotator indoamrbart-mbart-ft...</td>\n",
       "      <td>dan pasang-01 lampu keamanan juga menyapu-01 p...</td>\n",
       "      <td>pasang-01 orang memiliki-peran-org-91 polisi l...</td>\n",
       "      <td>[(z0, :instance, dan), (z0, :op1, z1), (z1, :i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td># ::id 102\\n# ::annotator indoamrbart-mbart-ft...</td>\n",
       "      <td># ::id 102\\n# ::annotator indoamrbart-mbart-ft...</td>\n",
       "      <td>umum-01 orang nama hanya menggantikan-01 orang...</td>\n",
       "      <td>hitung-01 partai besar orang nama memiliki-per...</td>\n",
       "      <td>[(z0, :instance, umum-01), (z0, :ARG0, z1), (z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td># ::id 103\\n# ::annotator indoamrbart-mbart-ft...</td>\n",
       "      <td># ::id 103\\n# ::annotator indoamrbart-mbart-ft...</td>\n",
       "      <td>katakan-01 dia mungkin-01 memperluas-01 serang...</td>\n",
       "      <td>katakan-01 orang nama mungkin-01 melebar-01 me...</td>\n",
       "      <td>[(z0, :instance, katakan-01), (z0, :ARG0, z1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td># ::id 104\\n# ::annotator indoamrbart-mbart-ft...</td>\n",
       "      <td># ::id 104\\n# ::annotator indoamrbart-mbart-ft...</td>\n",
       "      <td>luncurkan-01 orang nama orang berhasil-01 doku...</td>\n",
       "      <td>mungkin-01 ini dokumen apapun mendukung-01 pro...</td>\n",
       "      <td>[(z0, :instance, luncurkan-01), (z0, :ARG0, z1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td># ::id 105\\n# ::annotator indoamrbart-mbart-ft...</td>\n",
       "      <td># ::id 105\\n# ::annotator indoamrbart-mbart-ft...</td>\n",
       "      <td>katakan-01 orang nama memiliki-peran-org-91 pe...</td>\n",
       "      <td>memilukan-01 komunitas seluruh universitas nam...</td>\n",
       "      <td>[(z0, :instance, katakan-01), (z0, :ARG0, z1),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td># ::id 106\\n# ::annotator indoamrbart-mbart-ft...</td>\n",
       "      <td># ::id 106\\n# ::annotator indoamrbart-mbart-ft...</td>\n",
       "      <td>ujar-01 dia obligasi-01 aku bertemu-01 salah</td>\n",
       "      <td>obligasi-01 aku berada-01 bertemu-01 salah dan...</td>\n",
       "      <td>[(z0, :instance, ujar-01), (z0, :ARG0, z1), (z...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td># ::id 107\\n# ::annotator indoamrbart-mbart-ft...</td>\n",
       "      <td># ::id 107\\n# ::annotator indoamrbart-mbart-ft...</td>\n",
       "      <td>kirim-01 agen FBI selusin dan aman-01 bukti an...</td>\n",
       "      <td>kirim-01 orang nama memiliki-peran-org-91 orga...</td>\n",
       "      <td>[(z0, :instance, kirim-01), (z0, :ARG0, z1), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td># ::id 108\\n# ::annotator indoamrbart-mbart-ft...</td>\n",
       "      <td># ::id 108\\n# ::annotator indoamrbart-mbart-ft...</td>\n",
       "      <td>operasi-01 mereka operasi hanya kuantitas-seme...</td>\n",
       "      <td>kelangsungan-01 hidup kuantitas-sementara bula...</td>\n",
       "      <td>[(z0, :instance, operasi-01), (z0, :ARG0, z1),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  amr1  \\\n",
       "101  # ::id 101\\n# ::annotator indoamrbart-mbart-ft...   \n",
       "102  # ::id 102\\n# ::annotator indoamrbart-mbart-ft...   \n",
       "103  # ::id 103\\n# ::annotator indoamrbart-mbart-ft...   \n",
       "104  # ::id 104\\n# ::annotator indoamrbart-mbart-ft...   \n",
       "105  # ::id 105\\n# ::annotator indoamrbart-mbart-ft...   \n",
       "106  # ::id 106\\n# ::annotator indoamrbart-mbart-ft...   \n",
       "107  # ::id 107\\n# ::annotator indoamrbart-mbart-ft...   \n",
       "108  # ::id 108\\n# ::annotator indoamrbart-mbart-ft...   \n",
       "\n",
       "                                                  amr2  \\\n",
       "101  # ::id 101\\n# ::annotator indoamrbart-mbart-ft...   \n",
       "102  # ::id 102\\n# ::annotator indoamrbart-mbart-ft...   \n",
       "103  # ::id 103\\n# ::annotator indoamrbart-mbart-ft...   \n",
       "104  # ::id 104\\n# ::annotator indoamrbart-mbart-ft...   \n",
       "105  # ::id 105\\n# ::annotator indoamrbart-mbart-ft...   \n",
       "106  # ::id 106\\n# ::annotator indoamrbart-mbart-ft...   \n",
       "107  # ::id 107\\n# ::annotator indoamrbart-mbart-ft...   \n",
       "108  # ::id 108\\n# ::annotator indoamrbart-mbart-ft...   \n",
       "\n",
       "                                         concepts_amr1  \\\n",
       "101  dan pasang-01 lampu keamanan juga menyapu-01 p...   \n",
       "102  umum-01 orang nama hanya menggantikan-01 orang...   \n",
       "103  katakan-01 dia mungkin-01 memperluas-01 serang...   \n",
       "104  luncurkan-01 orang nama orang berhasil-01 doku...   \n",
       "105  katakan-01 orang nama memiliki-peran-org-91 pe...   \n",
       "106       ujar-01 dia obligasi-01 aku bertemu-01 salah   \n",
       "107  kirim-01 agen FBI selusin dan aman-01 bukti an...   \n",
       "108  operasi-01 mereka operasi hanya kuantitas-seme...   \n",
       "\n",
       "                                         concepts_amr2  \\\n",
       "101  pasang-01 orang memiliki-peran-org-91 polisi l...   \n",
       "102  hitung-01 partai besar orang nama memiliki-per...   \n",
       "103  katakan-01 orang nama mungkin-01 melebar-01 me...   \n",
       "104  mungkin-01 ini dokumen apapun mendukung-01 pro...   \n",
       "105  memilukan-01 komunitas seluruh universitas nam...   \n",
       "106  obligasi-01 aku berada-01 bertemu-01 salah dan...   \n",
       "107  kirim-01 orang nama memiliki-peran-org-91 orga...   \n",
       "108  kelangsungan-01 hidup kuantitas-sementara bula...   \n",
       "\n",
       "                                            merged_amr  \n",
       "101  [(z0, :instance, dan), (z0, :op1, z1), (z1, :i...  \n",
       "102  [(z0, :instance, umum-01), (z0, :ARG0, z1), (z...  \n",
       "103  [(z0, :instance, katakan-01), (z0, :ARG0, z1),...  \n",
       "104  [(z0, :instance, luncurkan-01), (z0, :ARG0, z1...  \n",
       "105  [(z0, :instance, katakan-01), (z0, :ARG0, z1),...  \n",
       "106  [(z0, :instance, ujar-01), (z0, :ARG0, z1), (z...  \n",
       "107  [(z0, :instance, kirim-01), (z0, :ARG0, z1), (...  \n",
       "108  [(z0, :instance, operasi-01), (z0, :ARG0, z1),...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_3_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('z0', ':instance', 'dan'), ('z0', ':op1', 'z1'), ('z1', ':instance', 'pasang-01'), ('z1', ':ARG1', 'z2'), ('z2', ':instance', 'lampu'), ('z2', ':mod', 'z3'), ('z3', ':instance', 'keamanan'), ('z1', ':time', 'z4'), ('z4', ':instance', 'juga'), ('z0', ':op2', 'z5'), ('z5', ':instance', 'menyapu-01'), ('z5', ':ARG0', 'z6'), ('z6', ':instance', 'polisi'), ('z5', ':ARG1', 'z7'), ('z7', ':instance', 'lahan'), ('z5', ':purpose', 'z8'), ('z8', ':instance', 'perangkap'), ('z8', ':mod', 'z9'), ('z9', ':instance', 'booby'), ('y1', ':instance', 'orang'), ('y2', ':instance', 'memiliki-peran-org-91'), ('y6', ':instance', 'gudang'), ('y7', ':instance', 'dekat'), ('y8', ':instance', 'pintu'), ('y9', ':instance', 'depan'), ('y2', ':ARG0', 'y1'), ('y6', ':location', 'y7'), ('y7', ':op1', 'y8'), ('y8', ':mod', 'y9'), ('z1', ':instance', 'pasang-01'), ('z1', ':ARG0', 'y1'), ('z1', ':location', 'y6'), ('z6', ':instance', 'polisi'), ('y2', ':ARG2', 'z6'), ('z2', ':instance', 'lampu'), ('z1', ':ARG1', 'z2'), ('z3', ':instance', 'keamanan'), ('z2', ':mod', 'z3'), ('z4', ':instance', 'juga'), ('z1', ':mod', 'z4')]\n"
     ]
    }
   ],
   "source": [
    "print(instance_3_a.iloc[0]['merged_amr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_3_a = instance_3_a.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_3_a = instance_3_a.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import vstack, hstack, csr_matrix\n",
    "import networkx as nx\n",
    "import re\n",
    "\n",
    "def calculate_pagerank(graph_merged):\n",
    "    G = nx.DiGraph()\n",
    "    for triple in graph_merged:\n",
    "        if(triple[1] != ':instance'):\n",
    "            G.add_edge(triple[0], triple[2])\n",
    "    pagerank = nx.pagerank(G, alpha=0.85, max_iter=1000)\n",
    "    return pagerank\n",
    "\n",
    "def extr_tf(df_train, df_test, factor=False, n_comp=100, inductive=True):\n",
    "    df_train = df_train.copy()\n",
    "    df_test = df_test.copy()\n",
    "    if inductive:\n",
    "        all_sent = np.concatenate([df_train['concepts_amr1'], df_train['concepts_amr2']])\n",
    "    else:\n",
    "        all_sent = np.concatenate([df_train['concepts_amr1'], df_train['concepts_amr2'],\n",
    "                                    df_test['concepts_amr1'], df_test['concepts_amr2']])\n",
    "    vec = CountVectorizer(token_pattern=r'\\b\\w[\\w-]*\\b')\n",
    "    vec.fit(all_sent)\n",
    "\n",
    "    X1_train = vec.transform(df_train['concepts_amr1'])\n",
    "    print(X1_train.toarray())\n",
    "    X2_train = vec.transform(df_train['concepts_amr2'])\n",
    "    print(X2_train.toarray())\n",
    "    X1_train = X1_train.astype(float)\n",
    "    X2_train = X2_train.astype(float)\n",
    "    for index, row in df_train.iterrows():\n",
    "        amr = row['merged_amr']        \n",
    "        pagerank = calculate_pagerank(amr)\n",
    "        for node in pagerank:\n",
    "            for triple in amr:\n",
    "                if ((triple[0] == node) and (triple[1] == \":instance\")) or triple[2]==node:\n",
    "                    word = triple[2]\n",
    "                    word = re.sub(r'[^A-Za-z0-9-]', '', word)\n",
    "                    word = word.lower()\n",
    "                    if word in vec.vocabulary_:\n",
    "                        loc_word = vec.vocabulary_[word]\n",
    "                        X1_train[index, loc_word] *= (pagerank[node])\n",
    "                        X2_train[index, loc_word] *= (pagerank[node])\n",
    "        \n",
    "    print(X1_train.toarray())\n",
    "    print(X2_train.toarray())\n",
    "    \n",
    "    X1_test = vec.transform(df_test['concepts_amr1'])\n",
    "    X2_test = vec.transform(df_test['concepts_amr2'])\n",
    "    X1_test = X1_test.astype(float)\n",
    "    X2_test = X2_test.astype(float)\n",
    "    for index, row in df_test.iterrows():\n",
    "        amr = row['merged_amr']        \n",
    "        pagerank = calculate_pagerank(amr)\n",
    "        for node in pagerank:\n",
    "            for triple in amr:\n",
    "                if ((triple[0] == node) and (triple[1] == \":instance\")) or triple[2]==node:\n",
    "                    word = triple[2]\n",
    "                    word = re.sub(r'[^A-Za-z0-9-]', '', word)\n",
    "                    word = word.lower()\n",
    "                    if word in vec.vocabulary_:\n",
    "                        loc_word = vec.vocabulary_[word]\n",
    "                        X1_test[index, loc_word] *= (pagerank[node])\n",
    "                        X2_test[index, loc_word] *= (pagerank[node])\n",
    "    \n",
    "    X_train = vstack((X1_train, X2_train))\n",
    "    if (factor):\n",
    "        n_components = math.floor(n_comp * len(vec.vocabulary_))\n",
    "    else:\n",
    "        n_components = n_comp\n",
    "    lsa = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "    X_train = lsa.fit_transform(X_train)\n",
    "    \n",
    "    X_train = np.vsplit(X_train, 2)\n",
    "    sum_res = X_train[0] + X_train[1]\n",
    "    print(X_train[0])\n",
    "    print(X_train[1])\n",
    "    diff_res = np.abs(X_train[0] - X_train[1])\n",
    "    feat_1_train = np.column_stack((sum_res, diff_res))\n",
    "    print(feat_1_train)\n",
    "    feat_1_train = csr_matrix(feat_1_train)\n",
    "\n",
    "    X1_test = lsa.transform(X1_test)\n",
    "    X2_test = lsa.transform(X2_test)\n",
    "    sum_res = X1_test + X2_test\n",
    "    diff_res = np.abs(X1_test - X2_test)\n",
    "    feat_1_test = np.column_stack((sum_res, diff_res))\n",
    "    feat_1_test = csr_matrix(feat_1_test)\n",
    "\n",
    "    return feat_1_train, feat_1_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 0 1 1 1 1 0 1 0 1 1 0 1]]\n",
      "[[0 0 1 1 1 1 1 0 1 1 0 1 1 0 1 1]]\n",
      "[[0.07919248 0.03610859 0.         0.         0.         0.00221301\n",
      "  0.00579037 0.05068739 0.00221301 0.         0.05145453 0.\n",
      "  0.00264757 0.05068739 0.         0.0043604 ]]\n",
      "[[0.         0.         0.07609448 0.12177809 0.04704269 0.00221301\n",
      "  0.00579037 0.         0.00221301 0.03610859 0.         0.06238864\n",
      "  0.00264757 0.         0.10078939 0.0043604 ]]\n",
      "[[0.0005942  0.12421862]]\n",
      "[[ 0.19559439 -0.00037736]]\n",
      "[[0.19618859 0.12384125 0.19500019 0.12459598]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/faizmuh/miniconda3/envs/skripsi/lib/python3.10/site-packages/scipy/sparse/_index.py:100: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_intXint(row, col, x.flat[0])\n"
     ]
    }
   ],
   "source": [
    "_, _ = extr_tf(instance_3_a, instance_3_a, False, 2, inductive=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skripsi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
